{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad9bbce742c4c5aa439fedaa4623f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c35ab47c9254f879f1dcbe6bcf22f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base-mlm\")  # or a code search variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code snippet and documentation/comment\n",
    "code = \"def add(a, b): return a + b\"\n",
    "documentation = \"Function to add two numbers\"\n",
    "\n",
    "# Tokenize code\n",
    "code_tokens = tokenizer.encode(code, return_tensors='pt')\n",
    "# Tokenize natural language comment\n",
    "doc_tokens = tokenizer.encode(documentation, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Get embeddings for code\n",
    "with torch.no_grad():\n",
    "    code_embeddings = model(code_tokens)[0]\n",
    "\n",
    "# Get embeddings for documentation/comment\n",
    "with torch.no_grad():\n",
    "    doc_embeddings = model(doc_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# loading CodeBERT\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# demo code snippets\n",
    "code_snippets = [\n",
    "    \"def add(a, b): return a + b\"\n",
    "]\n",
    "\n",
    "code_embedding_list = []\n",
    "\n",
    "for code_snippet in code_snippets:\n",
    "    # tokenzing the code snippet\n",
    "    code_tokens = tokenizer.encode(code_snippet, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        code_outputs = model(code_tokens)\n",
    "        code_embeddings = code_outputs.last_hidden_state\n",
    "        code_embedding = torch.mean(code_embeddings, dim=1).squeeze(0)\n",
    "    code_embedding_list.append(code_embedding.numpy())\n",
    "\n",
    "code_embedding_array = np.array(code_embedding_list)  # Shape: (n_samples, hidden_size)\n",
    "\n",
    "# creating faiss index\n",
    "d = code_embedding_array.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(code_embedding_array.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Code Snippet: def add(a, b): return a + b\n",
      "Matched Code Snippet: def add(a, b): return a + b\n",
      "Matched Code Snippet: def add(a, b): return a + b\n",
      "Matched Code Snippet: def add(a, b): return a + b\n",
      "Matched Code Snippet: def add(a, b): return a + b\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "query = \"Function to add two numbers\"\n",
    "\n",
    "# Tokenize and get embedding\n",
    "query_tokens = tokenizer.encode(query, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    query_outputs = model(query_tokens)\n",
    "    query_embeddings = query_outputs.last_hidden_state\n",
    "    query_embedding = torch.mean(query_embeddings, dim=1).squeeze(0).numpy()\n",
    "\n",
    "# Ensure the embedding is float32\n",
    "query_embedding = np.array([query_embedding.astype('float32')])  # Shape: (1, d)\n",
    "\n",
    "# Perform the search\n",
    "k = 5  # Number of nearest neighbors\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve the corresponding code snippets\n",
    "for idx in indices[0]:\n",
    "    print(f\"Matched Code Snippet: {code_snippets[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
