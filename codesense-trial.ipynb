{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f73b4eaabf436ca6a18edbec593f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python.zip:   8%|7         | 73.4M/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bf1642b0e04a4682b391bc369d90c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ea01a237d84187b52ab68a73939a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b5c20e0c3f4fb0ac58fd9541e051d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Python subset of CodeSearchNet with trust_remote_code=True\n",
    "dataset = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "\n",
    "# Explore the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame Head:\n",
      "  repository_name  func_path_in_repository                    func_name  \\\n",
      "0  proycon/pynlpl  pynlpl/formats/folia.py  AbstractElement.addidsuffix   \n",
      "1  proycon/pynlpl  pynlpl/formats/folia.py   AbstractElement.setparents   \n",
      "2  proycon/pynlpl  pynlpl/formats/folia.py       AbstractElement.setdoc   \n",
      "3  proycon/pynlpl  pynlpl/formats/folia.py      AbstractElement.hastext   \n",
      "4  proycon/pynlpl  pynlpl/formats/folia.py      AbstractElement.hasphon   \n",
      "\n",
      "                                   whole_func_string language  \\\n",
      "0  def addidsuffix(self, idsuffix, recursive = Tr...   python   \n",
      "1  def setparents(self):\\n        \"\"\"Correct all ...   python   \n",
      "2  def setdoc(self,newdoc):\\n        \"\"\"Set a dif...   python   \n",
      "3  def hastext(self,cls='current',strict=True, co...   python   \n",
      "4  def hasphon(self,cls='current',strict=True,cor...   python   \n",
      "\n",
      "                                    func_code_string  \\\n",
      "0  def addidsuffix(self, idsuffix, recursive = Tr...   \n",
      "1  def setparents(self):\\n        \"\"\"Correct all ...   \n",
      "2  def setdoc(self,newdoc):\\n        \"\"\"Set a dif...   \n",
      "3  def hastext(self,cls='current',strict=True, co...   \n",
      "4  def hasphon(self,cls='current',strict=True,cor...   \n",
      "\n",
      "                                    func_code_tokens  \\\n",
      "0  [def, addidsuffix, (, self, ,, idsuffix, ,, re...   \n",
      "1  [def, setparents, (, self, ), :, for, c, in, s...   \n",
      "2  [def, setdoc, (, self, ,, newdoc, ), :, self, ...   \n",
      "3  [def, hastext, (, self, ,, cls, =, 'current', ...   \n",
      "4  [def, hasphon, (, self, ,, cls, =, 'current', ...   \n",
      "\n",
      "                           func_documentation_string  \\\n",
      "0  Appends a suffix to this element's ID, and opt...   \n",
      "1  Correct all parent relations for elements with...   \n",
      "2  Set a different document. Usually no need to c...   \n",
      "3  Does this element have text (of the specified ...   \n",
      "4  Does this element have phonetic content (of th...   \n",
      "\n",
      "                           func_documentation_tokens split_name  \\\n",
      "0  [Appends, a, suffix, to, this, element, s, ID,...      train   \n",
      "1  [Correct, all, parent, relations, for, element...      train   \n",
      "2  [Set, a, different, document, ., Usually, no, ...      train   \n",
      "3  [Does, this, element, have, text, (, of, the, ...      train   \n",
      "4  [Does, this, element, have, phonetic, content,...      train   \n",
      "\n",
      "                                       func_code_url  \n",
      "0  https://github.com/proycon/pynlpl/blob/7707f69...  \n",
      "1  https://github.com/proycon/pynlpl/blob/7707f69...  \n",
      "2  https://github.com/proycon/pynlpl/blob/7707f69...  \n",
      "3  https://github.com/proycon/pynlpl/blob/7707f69...  \n",
      "4  https://github.com/proycon/pynlpl/blob/7707f69...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert each split to a pandas DataFrame\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "validation_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Display the first few rows of the training set\n",
    "print(\"Training DataFrame Head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate code snippets.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on 'func_code_string'\n",
    "initial_count = len(train_df)\n",
    "train_df.drop_duplicates(subset=['func_code_string'], inplace=True)\n",
    "final_count = len(train_df)\n",
    "print(f\"Removed {initial_count - final_count} duplicate code snippets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 samples with missing documentation.\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing documentation\n",
    "initial_count = len(train_df)\n",
    "train_df.dropna(subset=['func_documentation_string'], inplace=True)\n",
    "final_count = len(train_df)\n",
    "print(f\"Removed {initial_count - final_count} samples with missing documentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-Python samples: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify all entries are in Python\n",
    "non_python = train_df[train_df['language'] != 'python']\n",
    "print(f\"Number of non-Python samples: {len(non_python)}\")\n",
    "\n",
    "# Optionally, remove non-Python samples\n",
    "train_df = train_df[train_df['language'] == 'python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 10000 examples for development.\n"
     ]
    }
   ],
   "source": [
    "# Sample 10,000 examples for development\n",
    "sample_size = 10000\n",
    "train_sample = train_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled {len(train_sample)} examples for development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned training sample to a CSV file\n",
    "train_sample.to_csv('cleaned_train_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
